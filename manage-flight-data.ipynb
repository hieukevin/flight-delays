{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16da5f9b",
   "metadata": {},
   "source": [
    "I have csv file with data, that have these columns: Year,Month,DayofMonth,DayOfWeek,Carrier,OriginAirportID,OriginAirportName,OriginCity,OriginState,DestAirportID,DestAirportName,DestCity,DestState,CRSDepTime,DepDelay,DepDel15,CRSArrTime,ArrDelay,ArrDel15,Cancelled,\n",
    "Cleanse the data by identifying null values and replacing them with an appropriate value (zero in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f555506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for data cleansing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61456f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in /workspaces/flight-delays/data:\n",
      "  - processed\n",
      "  - flights.csv\n"
     ]
    }
   ],
   "source": [
    "# First, let's see what files are available in the data directory\n",
    "data_dir = '/workspaces/flight-delays/data'\n",
    "if os.path.exists(data_dir):\n",
    "    print(f\"Files in {data_dir}:\")\n",
    "    for file in os.listdir(data_dir):\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(f\"Data directory {data_dir} not found. Let's check the current directory:\")\n",
    "    for file in os.listdir('.'):\n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5a744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded flights.csv with shape: (271940, 20)\n",
      "\n",
      "Columns: ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'Carrier', 'OriginAirportID', 'OriginAirportName', 'OriginCity', 'OriginState', 'DestAirportID', 'DestAirportName', 'DestCity', 'DestState', 'CRSDepTime', 'DepDelay', 'DepDel15', 'CRSArrTime', 'ArrDelay', 'ArrDel15', 'Cancelled']\n"
     ]
    }
   ],
   "source": [
    "# Load the flights.csv file directly\n",
    "csv_path = '/workspaces/flight-delays/data/flights.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Successfully loaded flights.csv with shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643b6f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA CLEANSING ===\n",
      "Null values before cleaning:\n",
      "DepDel15    2761\n",
      "dtype: int64\n",
      "\n",
      "Null values after cleaning:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "✅ Success! All null values have been replaced with 0\n",
      "\n",
      "Cleaned dataset shape: (271940, 20)\n",
      "Series([], dtype: int64)\n",
      "\n",
      "✅ Success! All null values have been replaced with 0\n",
      "\n",
      "Cleaned dataset shape: (271940, 20)\n"
     ]
    }
   ],
   "source": [
    "# Data cleansing: identify null values and replace with 0\n",
    "print(\"=== DATA CLEANSING ===\")\n",
    "\n",
    "# Check for null values before cleaning\n",
    "print(\"Null values before cleaning:\")\n",
    "null_before = df.isnull().sum()\n",
    "print(null_before[null_before > 0])\n",
    "\n",
    "# Replace all null values with 0\n",
    "df_clean = df.fillna(0)\n",
    "\n",
    "# Verify cleaning was successful\n",
    "print(\"\\nNull values after cleaning:\")\n",
    "null_after = df_clean.isnull().sum()\n",
    "print(null_after[null_after > 0])\n",
    "\n",
    "if null_after.sum() == 0:\n",
    "    print(\"\\n✅ Success! All null values have been replaced with 0\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Warning: {null_after.sum()} null values still remain\")\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f40491d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleansed data saved to: /workspaces/flight-delays/data/processed/flights_clean.csv\n",
      "Original data: 271,940 rows\n",
      "Clean data: 271,940 rows\n",
      "Columns: 20\n",
      "\n",
      "Sample of cleaned data:\n",
      "   Year  Month  DayofMonth  DayOfWeek Carrier  OriginAirportID  \\\n",
      "0  2013      9          16          1      DL            15304   \n",
      "1  2013      9          23          1      WN            14122   \n",
      "2  2013      9           7          6      AS            14747   \n",
      "3  2013      7          22          1      OO            13930   \n",
      "4  2013      5          16          4      DL            13931   \n",
      "\n",
      "              OriginAirportName  OriginCity OriginState  DestAirportID  \\\n",
      "0           Tampa International       Tampa          FL          12478   \n",
      "1      Pittsburgh International  Pittsburgh          PA          13232   \n",
      "2  Seattle/Tacoma International     Seattle          WA          11278   \n",
      "3  Chicago O'Hare International     Chicago          IL          11042   \n",
      "4         Norfolk International     Norfolk          VA          10397   \n",
      "\n",
      "                            DestAirportName    DestCity DestState  CRSDepTime  \\\n",
      "0             John F. Kennedy International    New York        NY        1539   \n",
      "1              Chicago Midway International     Chicago        IL         710   \n",
      "2         Ronald Reagan Washington National  Washington        DC         810   \n",
      "3           Cleveland-Hopkins International   Cleveland        OH         804   \n",
      "4  Hartsfield-Jackson Atlanta International     Atlanta        GA         545   \n",
      "\n",
      "   DepDelay  DepDel15  CRSArrTime  ArrDelay  ArrDel15  Cancelled  \n",
      "0         4       0.0        1824        13         0          0  \n",
      "1         3       0.0         740        22         1          0  \n",
      "2        -3       0.0        1614        -7         0          0  \n",
      "3        35       1.0        1027        33         1          0  \n",
      "4        -1       0.0         728        -9         0          0  \n"
     ]
    }
   ],
   "source": [
    "# Save the cleansed data to a new file\n",
    "import os\n",
    "\n",
    "# Create processed data directory if it doesn't exist\n",
    "processed_dir = '/workspaces/flight-delays/data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save cleansed data\n",
    "clean_csv_path = os.path.join(processed_dir, 'flights_clean.csv')\n",
    "df_clean.to_csv(clean_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleansed data saved to: {clean_csv_path}\")\n",
    "print(f\"Original data: {df.shape[0]:,} rows\")\n",
    "print(f\"Clean data: {df_clean.shape[0]:,} rows\")\n",
    "print(f\"Columns: {df_clean.shape[1]}\")\n",
    "\n",
    "# Show a sample of the cleaned data\n",
    "print(f\"\\nSample of cleaned data:\")\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1296166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ENGINEERING ===\n",
      "Before filtering cancelled flights: 271,940 rows\n",
      "After filtering cancelled flights: 269,024 rows\n",
      "\n",
      "Feature engineering completed!\n",
      "New columns added: DepHour, ArrHour, DepTimeCategory, IsWeekend, Season\n",
      "Target variable: ArrDel15 (1 = delayed >15 minutes)\n",
      "\n",
      "Overall delay rate (>15 min): 20.7%\n",
      "Total flights for training: 269,024\n",
      "\n",
      "Feature engineering completed!\n",
      "New columns added: DepHour, ArrHour, DepTimeCategory, IsWeekend, Season\n",
      "Target variable: ArrDel15 (1 = delayed >15 minutes)\n",
      "\n",
      "Overall delay rate (>15 min): 20.7%\n",
      "Total flights for training: 269,024\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Feature Engineering for Delay Prediction Model\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Start with the clean data\n",
    "df_model = df_clean.copy()\n",
    "\n",
    "# Filter out cancelled flights (we're predicting delays, not cancellations)\n",
    "print(f\"Before filtering cancelled flights: {len(df_model):,} rows\")\n",
    "df_model = df_model[df_model['Cancelled'] == 0]\n",
    "print(f\"After filtering cancelled flights: {len(df_model):,} rows\")\n",
    "\n",
    "# Convert scheduled times to hour of day\n",
    "df_model['DepHour'] = (df_model['CRSDepTime'] // 100).astype(int)\n",
    "df_model['ArrHour'] = (df_model['CRSArrTime'] // 100).astype(int)\n",
    "\n",
    "# Fix any hour values that might be 24+ (should be rare)\n",
    "df_model['DepHour'] = df_model['DepHour'].clip(0, 23)\n",
    "df_model['ArrHour'] = df_model['ArrHour'].clip(0, 23)\n",
    "\n",
    "# Create time of day categories\n",
    "def get_time_category(hour):\n",
    "    if 6 <= hour <= 9:\n",
    "        return 'morning_rush'\n",
    "    elif 10 <= hour <= 16:\n",
    "        return 'midday'\n",
    "    elif 17 <= hour <= 20:\n",
    "        return 'evening_rush'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "df_model['DepTimeCategory'] = df_model['DepHour'].apply(get_time_category)\n",
    "\n",
    "# Create additional date features\n",
    "df_model['IsWeekend'] = (df_model['DayOfWeek'].isin([6, 7])).astype(int)\n",
    "df_model['Season'] = df_model['Month'].apply(lambda x: \n",
    "    'Winter' if x in [12, 1, 2] else\n",
    "    'Spring' if x in [3, 4, 5] else\n",
    "    'Summer' if x in [6, 7, 8] else\n",
    "    'Fall'\n",
    ")\n",
    "\n",
    "print(f\"\\nFeature engineering completed!\")\n",
    "print(f\"New columns added: DepHour, ArrHour, DepTimeCategory, IsWeekend, Season\")\n",
    "print(f\"Target variable: ArrDel15 (1 = delayed >15 minutes)\")\n",
    "\n",
    "# Show distribution of target variable\n",
    "delay_rate = df_model['ArrDel15'].mean()\n",
    "print(f\"\\nOverall delay rate (>15 min): {delay_rate:.1%}\")\n",
    "print(f\"Total flights for training: {len(df_model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b7b2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL PREPARATION ===\n",
      "Features: ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'OriginAirportID', 'DestAirportID', 'Carrier', 'DepHour', 'DepTimeCategory', 'IsWeekend', 'Season']\n",
      "Target: ArrDel15 (arrival delay >15 minutes)\n",
      "Dataset size: 269,024 flights, 11 features\n",
      "Encoded categorical features: ['Carrier', 'DepTimeCategory', 'Season']\n",
      "Training set: 215,219 flights\n",
      "Test set: 53,805 flights\n",
      "Training period delay rate: 20.8%\n",
      "Test period delay rate: 20.5%\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "import pickle\n",
    "\n",
    "print(\"=== MODEL PREPARATION ===\")\n",
    "\n",
    "# Select features for the model\n",
    "feature_columns = [\n",
    "    'Year', 'Month', 'DayofMonth', 'DayOfWeek', \n",
    "    'OriginAirportID', 'DestAirportID', \n",
    "    'Carrier', 'DepHour', 'DepTimeCategory', 'IsWeekend', 'Season'\n",
    "]\n",
    "\n",
    "# Prepare the dataset\n",
    "X = df_model[feature_columns].copy()\n",
    "y = df_model['ArrDel15'].copy()\n",
    "\n",
    "print(f\"Features: {feature_columns}\")\n",
    "print(f\"Target: ArrDel15 (arrival delay >15 minutes)\")\n",
    "print(f\"Dataset size: {X.shape[0]:,} flights, {X.shape[1]} features\")\n",
    "\n",
    "# Handle categorical variables\n",
    "categorical_features = ['Carrier', 'DepTimeCategory', 'Season']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"Encoded categorical features: {categorical_features}\")\n",
    "\n",
    "# Time-based split: use earlier data for training, later for testing\n",
    "# This mimics real-world prediction where we predict future flights\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} flights\")\n",
    "print(f\"Test set: {len(X_test):,} flights\")\n",
    "print(f\"Training period delay rate: {y_train.mean():.1%}\")\n",
    "print(f\"Test period delay rate: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f25e3393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL TRAINING ===\n",
      "Training Logistic Regression model...\n",
      "Training Random Forest model...\n",
      "Training Random Forest model...\n",
      "\n",
      "=== MODEL EVALUATION ===\n",
      "\n",
      "Logistic Regression Results:\n",
      "  Accuracy: 0.599\n",
      "  ROC-AUC: 0.641\n",
      "  Avg predicted delay probability: 48.3%\n",
      "\n",
      "Random Forest Results:\n",
      "\n",
      "=== MODEL EVALUATION ===\n",
      "\n",
      "Logistic Regression Results:\n",
      "  Accuracy: 0.599\n",
      "  ROC-AUC: 0.641\n",
      "  Avg predicted delay probability: 48.3%\n",
      "\n",
      "Random Forest Results:\n",
      "  Accuracy: 0.795\n",
      "  ROC-AUC: 0.695\n",
      "  Avg predicted delay probability: 20.7%\n",
      "\n",
      "✅ Selected Random Forest as the final model\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "           Feature  Importance\n",
      "5    DestAirportID    0.245576\n",
      "4  OriginAirportID    0.236843\n",
      "2       DayofMonth    0.142459\n",
      "6          Carrier    0.127954\n",
      "7          DepHour    0.111134\n",
      "  Accuracy: 0.795\n",
      "  ROC-AUC: 0.695\n",
      "  Avg predicted delay probability: 20.7%\n",
      "\n",
      "✅ Selected Random Forest as the final model\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "           Feature  Importance\n",
      "5    DestAirportID    0.245576\n",
      "4  OriginAirportID    0.236843\n",
      "2       DayofMonth    0.142459\n",
      "6          Carrier    0.127954\n",
      "7          DepHour    0.111134\n"
     ]
    }
   ],
   "source": [
    "# Train delay prediction models\n",
    "print(\"=== MODEL TRAINING ===\")\n",
    "\n",
    "# Model 1: Logistic Regression (simple, interpretable)\n",
    "print(\"Training Logistic Regression model...\")\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Model 2: Random Forest (more complex, handles interactions)\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"\\n=== MODEL EVALUATION ===\")\n",
    "\n",
    "for name, model in [(\"Logistic Regression\", lr_model), (\"Random Forest\", rf_model)]:\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of delay\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc:.3f}\")\n",
    "    print(f\"  Avg predicted delay probability: {y_pred_proba.mean():.1%}\")\n",
    "\n",
    "# Choose the best model (Random Forest typically performs better)\n",
    "best_model = rf_model\n",
    "print(f\"\\n✅ Selected Random Forest as the final model\")\n",
    "\n",
    "# Show feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': best_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0679749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAVING MODEL FOR EXTERNAL USE ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to: /workspaces/flight-delays/server/model.pkl\n",
      "Model version: 1.0\n",
      "Training date: 2025-08-28\n",
      "Test accuracy: 0.795\n",
      "Test ROC-AUC: 0.695\n",
      "\n",
      "=== TESTING SAVED MODEL ===\n",
      "Successfully loaded model from file!\n",
      "Required input features: ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'OriginAirportID', 'DestAirportID', 'Carrier', 'DepHour', 'DepTimeCategory', 'IsWeekend', 'Season']\n",
      "Successfully loaded model from file!\n",
      "Required input features: ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'OriginAirportID', 'DestAirportID', 'Carrier', 'DepHour', 'DepTimeCategory', 'IsWeekend', 'Season']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'Winter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/_encode.py:164\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/_encode.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/_encode.py:158\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Winter'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probability\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Test with example\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m example_prob \u001b[38;5;241m=\u001b[39m predict_delay_probability(\n\u001b[1;32m     75\u001b[0m     year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2023\u001b[39m, month\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, day_of_month\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, day_of_week\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# Friday in December\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     origin_airport_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12892\u001b[39m, dest_airport_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14771\u001b[39m,  \u001b[38;5;66;03m# Example airport pair\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     carrier\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAA\u001b[39m\u001b[38;5;124m'\u001b[39m, departure_hour\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m17\u001b[39m  \u001b[38;5;66;03m# American Airlines, 5 PM departure\u001b[39;00m\n\u001b[1;32m     78\u001b[0m )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExample prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFriday Dec 15, 2023, AA flight departing 5 PM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 67\u001b[0m, in \u001b[0;36mpredict_delay_probability\u001b[0;34m(year, month, day_of_month, day_of_week, origin_airport_id, dest_airport_id, carrier, departure_hour)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Apply same preprocessing\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m loaded_pipeline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_features\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 67\u001b[0m     input_data[col] \u001b[38;5;241m=\u001b[39m loaded_pipeline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_encoders\u001b[39m\u001b[38;5;124m'\u001b[39m][col]\u001b[38;5;241m.\u001b[39mtransform(input_data[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m     70\u001b[0m probability \u001b[38;5;241m=\u001b[39m loaded_pipeline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict_proba(input_data[loaded_pipeline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_columns\u001b[39m\u001b[38;5;124m'\u001b[39m]])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:139\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/_encode.py:226\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'Winter'"
     ]
    }
   ],
   "source": [
    "# Save the complete model pipeline for external use\n",
    "print(\"=== SAVING MODEL FOR EXTERNAL USE ===\")\n",
    "\n",
    "# Create a complete pipeline object that includes preprocessing and model\n",
    "model_pipeline = {\n",
    "    'model': best_model,\n",
    "    'label_encoders': label_encoders,\n",
    "    'feature_columns': feature_columns,\n",
    "    'categorical_features': categorical_features,\n",
    "    'model_version': '1.0',\n",
    "    'trained_date': pd.Timestamp.now().strftime('%Y-%m-%d'),\n",
    "    'training_size': len(X_train),\n",
    "    'test_accuracy': accuracy_score(y_test, best_model.predict(X_test)),\n",
    "    'test_roc_auc': roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "\n",
    "# Save to the server directory (matches your repo structure)\n",
    "server_dir = '/workspaces/flight-delays/server'\n",
    "os.makedirs(server_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(server_dir, 'model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model_pipeline, f)\n",
    "\n",
    "print(f\"✅ Model saved to: {model_path}\")\n",
    "print(f\"Model version: {model_pipeline['model_version']}\")\n",
    "print(f\"Training date: {model_pipeline['trained_date']}\")\n",
    "print(f\"Test accuracy: {model_pipeline['test_accuracy']:.3f}\")\n",
    "print(f\"Test ROC-AUC: {model_pipeline['test_roc_auc']:.3f}\")\n",
    "\n",
    "# Test the saved model by loading it back\n",
    "print(f\"\\n=== TESTING SAVED MODEL ===\")\n",
    "with open(model_path, 'rb') as f:\n",
    "    loaded_pipeline = pickle.load(f)\n",
    "\n",
    "print(f\"Successfully loaded model from file!\")\n",
    "print(f\"Required input features: {loaded_pipeline['feature_columns']}\")\n",
    "\n",
    "# Example prediction function for external applications\n",
    "def predict_delay_probability(year, month, day_of_month, day_of_week, \n",
    "                            origin_airport_id, dest_airport_id, carrier, \n",
    "                            departure_hour):\n",
    "    \"\"\"\n",
    "    Predict probability of arrival delay >15 minutes\n",
    "    \n",
    "    Returns: probability (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    # Create input data\n",
    "    input_data = pd.DataFrame({\n",
    "        'Year': [year],\n",
    "        'Month': [month], \n",
    "        'DayofMonth': [day_of_month],\n",
    "        'DayOfWeek': [day_of_week],\n",
    "        'OriginAirportID': [origin_airport_id],\n",
    "        'DestAirportID': [dest_airport_id],\n",
    "        'Carrier': [carrier],\n",
    "        'DepHour': [departure_hour],\n",
    "        'DepTimeCategory': [get_time_category(departure_hour)],\n",
    "        'IsWeekend': [1 if day_of_week in [6, 7] else 0],\n",
    "        'Season': ['Winter' if month in [12, 1, 2] else\n",
    "                  'Spring' if month in [3, 4, 5] else\n",
    "                  'Summer' if month in [6, 7, 8] else 'Fall']\n",
    "    })\n",
    "    \n",
    "    # Apply same preprocessing\n",
    "    for col in loaded_pipeline['categorical_features']:\n",
    "        input_data[col] = loaded_pipeline['label_encoders'][col].transform(input_data[col].astype(str))\n",
    "    \n",
    "    # Predict\n",
    "    probability = loaded_pipeline['model'].predict_proba(input_data[loaded_pipeline['feature_columns']])[0][1]\n",
    "    return probability\n",
    "\n",
    "# Test with example\n",
    "example_prob = predict_delay_probability(\n",
    "    year=2023, month=12, day_of_month=15, day_of_week=5,  # Friday in December\n",
    "    origin_airport_id=12892, dest_airport_id=14771,  # Example airport pair\n",
    "    carrier='AA', departure_hour=17  # American Airlines, 5 PM departure\n",
    ")\n",
    "\n",
    "print(f\"\\nExample prediction:\")\n",
    "print(f\"Friday Dec 15, 2023, AA flight departing 5 PM\")\n",
    "print(f\"Predicted delay probability: {example_prob:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d10cc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING AIRPORTS LOOKUP FILE ===\n",
      "Using df_clean from previous cells\n",
      "Total airport records before deduplication: 543,880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final unique airports: 70\n",
      "✅ Airports lookup saved to: /workspaces/flight-delays/data/airports.csv\n",
      "\n",
      "Sample of airports data:\n",
      "   AirportID                                        AirportName\n",
      "0      10140                  Albuquerque International Sunport\n",
      "1      10423                   Austin - Bergstrom International\n",
      "2      10821  Baltimore/Washington International Thurgood Ma...\n",
      "3      10800                                           Bob Hope\n",
      "4      10529                              Bradley International\n",
      "5      10792                      Buffalo Niagara International\n",
      "6      11057                    Charlotte Douglas International\n",
      "7      13232                       Chicago Midway International\n",
      "8      13930                       Chicago O'Hare International\n",
      "9      11193         Cincinnati/Northern Kentucky International\n",
      "✅ Airports lookup also saved as JSON: /workspaces/flight-delays/data/airports.json\n",
      "\n",
      "=== SUMMARY ===\n",
      "✅ Data cleansing completed: 271,940 flights\n",
      "✅ Model trained and saved: /workspaces/flight-delays/server/model.pkl\n",
      "✅ Airports lookup created: 70 unique airports\n",
      "✅ All files ready for external applications!\n",
      "\n",
      "Files created:\n",
      "  - /workspaces/flight-delays/data/processed/flights_clean.csv (cleaned flight data)\n",
      "  - /workspaces/flight-delays/server/model.pkl (trained model)\n",
      "  - /workspaces/flight-delays/data/airports.csv (airports CSV)\n",
      "  - /workspaces/flight-delays/data/airports.json (airports JSON)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create airports lookup file with names and IDs\n",
    "print(\"=== CREATING AIRPORTS LOOKUP FILE ===\")\n",
    "\n",
    "# Load the cleaned data if not already available\n",
    "try:\n",
    "    df_clean\n",
    "    print(\"Using df_clean from previous cells\")\n",
    "except NameError:\n",
    "    print(\"Loading cleaned data from file...\")\n",
    "    clean_csv_path = '/workspaces/flight-delays/data/processed/flights_clean.csv'\n",
    "    df_clean = pd.read_csv(clean_csv_path)\n",
    "    print(f\"Loaded cleaned data: {df_clean.shape}\")\n",
    "\n",
    "# Extract unique airports from both origin and destination columns\n",
    "origin_airports = df_clean[['OriginAirportID', 'OriginAirportName']].rename(columns={\n",
    "    'OriginAirportID': 'AirportID',\n",
    "    'OriginAirportName': 'AirportName'\n",
    "})\n",
    "\n",
    "dest_airports = df_clean[['DestAirportID', 'DestAirportName']].rename(columns={\n",
    "    'DestAirportID': 'AirportID', \n",
    "    'DestAirportName': 'AirportName'\n",
    "})\n",
    "\n",
    "# Combine and deduplicate\n",
    "all_airports = pd.concat([origin_airports, dest_airports], ignore_index=True)\n",
    "\n",
    "print(f\"Total airport records before deduplication: {len(all_airports):,}\")\n",
    "\n",
    "# Remove duplicates and handle conflicts\n",
    "# Group by AirportID and take the most frequent name for each ID\n",
    "airports_grouped = all_airports.groupby('AirportID')['AirportName'].agg(['first', 'nunique']).reset_index()\n",
    "airports_grouped.columns = ['AirportID', 'AirportName', 'NameVariations']\n",
    "\n",
    "# Check for airports with multiple names\n",
    "name_conflicts = airports_grouped[airports_grouped['NameVariations'] > 1]\n",
    "if len(name_conflicts) > 0:\n",
    "    print(f\"Found {len(name_conflicts)} airports with multiple name variations:\")\n",
    "    print(name_conflicts.head())\n",
    "\n",
    "# Create final clean airports list\n",
    "airports_final = airports_grouped[['AirportID', 'AirportName']].copy()\n",
    "airports_final = airports_final.sort_values('AirportName').reset_index(drop=True)\n",
    "\n",
    "print(f\"Final unique airports: {len(airports_final):,}\")\n",
    "\n",
    "# Save airports lookup file\n",
    "airports_path = '/workspaces/flight-delays/data/airports.csv'\n",
    "airports_final.to_csv(airports_path, index=False)\n",
    "\n",
    "print(f\"✅ Airports lookup saved to: {airports_path}\")\n",
    "print(f\"\\nSample of airports data:\")\n",
    "print(airports_final.head(10))\n",
    "\n",
    "# Also create a JSON version for web applications\n",
    "airports_json_path = '/workspaces/flight-delays/data/airports.json'\n",
    "airports_dict = airports_final.to_dict('records')\n",
    "with open(airports_json_path, 'w') as f:\n",
    "    json.dump(airports_dict, f, indent=2)\n",
    "\n",
    "print(f\"✅ Airports lookup also saved as JSON: {airports_json_path}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"✅ Data cleansing completed: {len(df_clean):,} flights\")\n",
    "print(f\"✅ Model trained and saved: {model_path}\")\n",
    "print(f\"✅ Airports lookup created: {len(airports_final):,} unique airports\")\n",
    "print(f\"✅ All files ready for external applications!\")\n",
    "\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - {clean_csv_path} (cleaned flight data)\")\n",
    "print(f\"  - {model_path} (trained model)\")\n",
    "print(f\"  - {airports_path} (airports CSV)\")\n",
    "print(f\"  - {airports_json_path} (airports JSON)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
